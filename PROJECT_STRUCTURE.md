# Project Structure

## What the Dream Obscures
### A Counterfactual Exploration of MoMA's Archive

---

## ðŸ“ Clean Project Structure

```
windsurf-project/
â”œâ”€â”€ README.md                    # Main documentation
â”œâ”€â”€ NEW_PROJECT_PLAN.md          # Detailed architecture & concept
â”œâ”€â”€ FINAL_SETUP.md               # Setup guide & usage
â”œâ”€â”€ QUICK_REFERENCE.md           # Quick commands & tips
â”œâ”€â”€ PROJECT_STRUCTURE.md         # This file
â”‚
â”œâ”€â”€ .env                         # API keys (not in git)
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚
â”œâ”€â”€ build_latent_space.py        # Creates neural network embeddings
â”œâ”€â”€ app.py                       # Flask web server
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ artworks/
â”‚       â””â”€â”€ moma_data/
â”‚           â”œâ”€â”€ Artworks.json    # 158,863 artworks
â”‚           â”œâ”€â”€ Artworks.txt
â”‚           â”œâ”€â”€ Artists.json     # 15,763 artists
â”‚           â””â”€â”€ Artists.txt
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ latent_space/            # Generated embeddings (created by build script)
â”‚       â”œâ”€â”€ embeddings_full.npy
â”‚       â”œâ”€â”€ embeddings_reduced.npy
â”‚       â”œâ”€â”€ artworks.json
â”‚       â”œâ”€â”€ descriptions.json
â”‚       â”œâ”€â”€ metadata.json
â”‚       â””â”€â”€ statistics.json
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html               # Main web interface
â”‚
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css            # Styling
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ main.js              # Interactivity
â”‚
â””â”€â”€ venv/                        # Python virtual environment
```

---

## ðŸŽ¯ Key Files Explained

### Core Application

**`app.py`** - Flask backend server
- API endpoints for navigation
- Image generation via Replicate
- Statistics computation
- Dimension-based filtering

**`build_latent_space.py`** - Data processing
- Loads MoMA collection
- Creates text descriptions from metadata
- Generates neural network embeddings
- Builds navigable latent space

### Frontend

**`templates/index.html`** - Web interface
- Interactive navigation UI
- Metadata display
- Image generation controls
- Statistics dashboard

**`static/js/main.js`** - JavaScript logic
- API calls
- User interactions
- Dynamic content updates

**`static/css/style.css`** - Styling
- Dark theme
- Responsive design
- Visual components

### Documentation

**`README.md`** - Main documentation
- Project overview
- Quick start guide
- Technical details

**`NEW_PROJECT_PLAN.md`** - Detailed plan
- Complete architecture
- Critical framework
- Implementation strategy

**`FINAL_SETUP.md`** - Setup guide
- Installation steps
- Usage instructions
- Troubleshooting

**`QUICK_REFERENCE.md`** - Quick reference
- Commands
- Key concepts
- Troubleshooting tips

### Configuration

**`.env`** - Environment variables
- `REPLICATE_API_TOKEN` - Your API key
- Not committed to git

**`requirements.txt`** - Python packages
- All dependencies listed
- Install with: `pip install -r requirements.txt`

---

## ðŸš€ Workflow

### 1. Setup (One Time)
```bash
# Install dependencies
./venv/bin/pip install -r requirements.txt

# Add API key to .env
echo "REPLICATE_API_TOKEN=your_token" > .env
```

### 2. Build Latent Space (One Time)
```bash
# Creates embeddings from MoMA data
./venv/bin/python build_latent_space.py

# Takes ~15-20 minutes
# Output: outputs/latent_space/
```

### 3. Run Application
```bash
# Launch web server
./venv/bin/python app.py

# Open: http://localhost:5000
```

---

## ðŸ“Š Data Flow

```
MoMA JSON Files
    â†“
build_latent_space.py
    â†“
Text Descriptions (metadata â†’ text)
    â†“
Neural Network Embeddings (sentence-transformers)
    â†“
Latent Space (384D â†’ 50D via PCA)
    â†“
Saved to outputs/latent_space/
    â†“
app.py loads embeddings
    â†“
User navigates via web interface
    â†“
API calls for navigation & generation
    â†“
Replicate generates images
    â†“
Statistics computed in real-time
```

---

## ðŸŽ¨ User Journey

```
1. Visit http://localhost:5000
2. Click "Start Exploring"
3. See random artwork + metadata
4. Click "Generate Visual from Metadata"
5. Choose navigation direction
6. Select next artwork from neighbors
7. Repeat steps 3-6
8. Click "Show Statistics"
9. See biases in your path
```

---

## ðŸ’° Costs

- **Setup**: Free
- **Building latent space**: Free (local compute)
- **Per image generation**: ~$0.002
- **Typical session**: ~$0.04 (20 images)
- **For paper**: < $1 (50 screenshots)

---

## ðŸ”§ Maintenance

### Rebuild Latent Space
```bash
rm -rf outputs/latent_space
./venv/bin/python build_latent_space.py
```

### Update Dependencies
```bash
./venv/bin/pip install --upgrade -r requirements.txt
```

### Clear Cache
```bash
# Clear generated images cache
# (They're stored in memory, restart app.py)
```

---

## ðŸ“ For Your Paper

### Files to Reference

1. **`NEW_PROJECT_PLAN.md`** - Cite architecture
2. **`app.py`** - Cite implementation
3. **`outputs/latent_space/statistics.json`** - Cite dataset stats
4. **Screenshots from web interface** - Visual evidence

### Key Statistics

From `outputs/latent_space/statistics.json`:
- Total artworks in latent space
- Nationality distribution
- Gender distribution
- Top represented countries/artists

---

## âœ¨ What Makes This Work

**Technical**:
- Neural network embeddings capture semantic similarity
- PCA reduces dimensions while preserving structure
- Cosine similarity finds nearest neighbors
- Dimension filtering reveals biases

**Critical**:
- Metadata visibility counters abstraction
- Discrete choices counter smooth interpolation
- Statistics quantify biases
- Generation shows interpretation

**Experiential**:
- Users actively participate
- Choices have consequences
- Biases become visible
- Archive becomes navigable

---

**This is your complete, clean project structure!** ðŸŽ¨

All old files removed. Only the counterfactual exploration remains.
